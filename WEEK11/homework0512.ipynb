{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd4fff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdsets\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253a561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (2.1.3)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0dd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbe20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b7e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 3.33MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 152kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.08MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.94MB/s]\n"
     ]
    }
   ],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "data_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb8a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(28*28, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86db9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f60c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 0, Loss: 2.3099\n",
      "Epoch: 1, Step: 100, Loss: 2.2742\n",
      "Epoch: 1, Step: 200, Loss: 2.2044\n",
      "Epoch: 1, Step: 300, Loss: 2.1298\n",
      "Epoch: 1, Step: 400, Loss: 1.9709\n",
      "Epoch: 1, Step: 500, Loss: 1.7315\n",
      "Epoch: 2, Step: 0, Loss: 1.4939\n",
      "Epoch: 2, Step: 100, Loss: 1.1604\n",
      "Epoch: 2, Step: 200, Loss: 0.9468\n",
      "Epoch: 2, Step: 300, Loss: 0.8447\n",
      "Epoch: 2, Step: 400, Loss: 0.7137\n",
      "Epoch: 2, Step: 500, Loss: 0.5731\n",
      "Epoch: 3, Step: 0, Loss: 0.5998\n",
      "Epoch: 3, Step: 100, Loss: 0.5531\n",
      "Epoch: 3, Step: 200, Loss: 0.4878\n",
      "Epoch: 3, Step: 300, Loss: 0.6428\n",
      "Epoch: 3, Step: 400, Loss: 0.4676\n",
      "Epoch: 3, Step: 500, Loss: 0.4466\n",
      "Epoch: 4, Step: 0, Loss: 0.3453\n",
      "Epoch: 4, Step: 100, Loss: 0.5235\n",
      "Epoch: 4, Step: 200, Loss: 0.5095\n",
      "Epoch: 4, Step: 300, Loss: 0.3342\n",
      "Epoch: 4, Step: 400, Loss: 0.3295\n",
      "Epoch: 4, Step: 500, Loss: 0.3533\n",
      "Epoch: 5, Step: 0, Loss: 0.2631\n",
      "Epoch: 5, Step: 100, Loss: 0.3399\n",
      "Epoch: 5, Step: 200, Loss: 0.2993\n",
      "Epoch: 5, Step: 300, Loss: 0.3670\n",
      "Epoch: 5, Step: 400, Loss: 0.2979\n",
      "Epoch: 5, Step: 500, Loss: 0.4256\n",
      "Epoch: 6, Step: 0, Loss: 0.3511\n",
      "Epoch: 6, Step: 100, Loss: 0.4289\n",
      "Epoch: 6, Step: 200, Loss: 0.3620\n",
      "Epoch: 6, Step: 300, Loss: 0.2322\n",
      "Epoch: 6, Step: 400, Loss: 0.2949\n",
      "Epoch: 6, Step: 500, Loss: 0.5616\n",
      "Epoch: 7, Step: 0, Loss: 0.2206\n",
      "Epoch: 7, Step: 100, Loss: 0.3003\n",
      "Epoch: 7, Step: 200, Loss: 0.4549\n",
      "Epoch: 7, Step: 300, Loss: 0.2457\n",
      "Epoch: 7, Step: 400, Loss: 0.2488\n",
      "Epoch: 7, Step: 500, Loss: 0.3886\n",
      "Epoch: 8, Step: 0, Loss: 0.2724\n",
      "Epoch: 8, Step: 100, Loss: 0.1881\n",
      "Epoch: 8, Step: 200, Loss: 0.2544\n",
      "Epoch: 8, Step: 300, Loss: 0.2487\n",
      "Epoch: 8, Step: 400, Loss: 0.3335\n",
      "Epoch: 8, Step: 500, Loss: 0.3662\n",
      "Epoch: 9, Step: 0, Loss: 0.2476\n",
      "Epoch: 9, Step: 100, Loss: 0.2541\n",
      "Epoch: 9, Step: 200, Loss: 0.2604\n",
      "Epoch: 9, Step: 300, Loss: 0.2893\n",
      "Epoch: 9, Step: 400, Loss: 0.2436\n",
      "Epoch: 9, Step: 500, Loss: 0.2712\n",
      "Epoch: 10, Step: 0, Loss: 0.3001\n",
      "Epoch: 10, Step: 100, Loss: 0.1602\n",
      "Epoch: 10, Step: 200, Loss: 0.1976\n",
      "Epoch: 10, Step: 300, Loss: 0.2144\n",
      "Epoch: 10, Step: 400, Loss: 0.2829\n",
      "Epoch: 10, Step: 500, Loss: 0.2786\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    for j, [image, label] in enumerate(data_loader):\n",
    "        output = model(image)\n",
    "        cost = loss(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            print(f'Epoch: {i+1}, Step: {j}, Loss: {cost.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "038891d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dnn(model, test_loader):\n",
    "    model.eval()  # 평가 모드로 전환\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'✅ [DNN] 테스트 정확도: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85d0735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [DNN] 테스트 정확도: 97.97%\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(dataset=mnist_test, batch_size=100, shuffle=False)\n",
    "evaluate_dnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aeae960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_dnn_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for i in range(5):\n",
    "        img = images[i].reshape(28, 28)  # DNN에서는 Flatten된 이미지라서 다시 2D로\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'Pred: {predicted[i].item()}\\nLabel: {labels[i].item()}')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a972e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dnn_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba74e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2002a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a1f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test = dsets.MNIST(root='MNIST_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac92abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_layer = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(64*7*7, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090cccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b8080a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 0, Loss: 0.0489\n",
      "Epoch: 1, Step: 100, Loss: 0.0875\n",
      "Epoch: 1, Step: 200, Loss: 0.1000\n",
      "Epoch: 1, Step: 300, Loss: 0.0841\n",
      "Epoch: 1, Step: 400, Loss: 0.0542\n",
      "Epoch: 1, Step: 500, Loss: 0.0389\n",
      "Epoch: 2, Step: 0, Loss: 0.0123\n",
      "Epoch: 2, Step: 100, Loss: 0.0176\n",
      "Epoch: 2, Step: 200, Loss: 0.0588\n",
      "Epoch: 2, Step: 300, Loss: 0.0797\n",
      "Epoch: 2, Step: 400, Loss: 0.1218\n",
      "Epoch: 2, Step: 500, Loss: 0.0692\n",
      "Epoch: 3, Step: 0, Loss: 0.0278\n",
      "Epoch: 3, Step: 100, Loss: 0.1118\n",
      "Epoch: 3, Step: 200, Loss: 0.0416\n",
      "Epoch: 3, Step: 300, Loss: 0.0250\n",
      "Epoch: 3, Step: 400, Loss: 0.0425\n",
      "Epoch: 3, Step: 500, Loss: 0.0228\n",
      "Epoch: 4, Step: 0, Loss: 0.0640\n",
      "Epoch: 4, Step: 100, Loss: 0.0352\n",
      "Epoch: 4, Step: 200, Loss: 0.0823\n",
      "Epoch: 4, Step: 300, Loss: 0.0067\n",
      "Epoch: 4, Step: 400, Loss: 0.0466\n",
      "Epoch: 4, Step: 500, Loss: 0.0095\n",
      "Epoch: 5, Step: 0, Loss: 0.0406\n",
      "Epoch: 5, Step: 100, Loss: 0.0083\n",
      "Epoch: 5, Step: 200, Loss: 0.0662\n",
      "Epoch: 5, Step: 300, Loss: 0.0712\n",
      "Epoch: 5, Step: 400, Loss: 0.1541\n",
      "Epoch: 5, Step: 500, Loss: 0.0410\n",
      "Epoch: 6, Step: 0, Loss: 0.0032\n",
      "Epoch: 6, Step: 100, Loss: 0.0189\n",
      "Epoch: 6, Step: 200, Loss: 0.0802\n",
      "Epoch: 6, Step: 300, Loss: 0.0186\n",
      "Epoch: 6, Step: 400, Loss: 0.0309\n",
      "Epoch: 6, Step: 500, Loss: 0.0641\n",
      "Epoch: 7, Step: 0, Loss: 0.1218\n",
      "Epoch: 7, Step: 100, Loss: 0.1077\n",
      "Epoch: 7, Step: 200, Loss: 0.1087\n",
      "Epoch: 7, Step: 300, Loss: 0.0625\n",
      "Epoch: 7, Step: 400, Loss: 0.1474\n",
      "Epoch: 7, Step: 500, Loss: 0.1441\n",
      "Epoch: 8, Step: 0, Loss: 0.0667\n",
      "Epoch: 8, Step: 100, Loss: 0.0421\n",
      "Epoch: 8, Step: 200, Loss: 0.0255\n",
      "Epoch: 8, Step: 300, Loss: 0.0285\n",
      "Epoch: 8, Step: 400, Loss: 0.0159\n",
      "Epoch: 8, Step: 500, Loss: 0.0153\n",
      "Epoch: 9, Step: 0, Loss: 0.0439\n",
      "Epoch: 9, Step: 100, Loss: 0.0387\n",
      "Epoch: 9, Step: 200, Loss: 0.0266\n",
      "Epoch: 9, Step: 300, Loss: 0.0968\n",
      "Epoch: 9, Step: 400, Loss: 0.0790\n",
      "Epoch: 9, Step: 500, Loss: 0.0143\n",
      "Epoch: 10, Step: 0, Loss: 0.0757\n",
      "Epoch: 10, Step: 100, Loss: 0.0899\n",
      "Epoch: 10, Step: 200, Loss: 0.0892\n",
      "Epoch: 10, Step: 300, Loss: 0.0198\n",
      "Epoch: 10, Step: 400, Loss: 0.1260\n",
      "Epoch: 10, Step: 500, Loss: 0.0946\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    for j, [image, label] in enumerate(train_loader):\n",
    "        output = model(image)\n",
    "        cost = loss(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            print(f'Epoch: {i+1}, Step: {j}, Loss: {cost.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b57f7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 측정 함수\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # 평가 모드\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'✅ 테스트 정확도: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f0301ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 테스트 정확도: 97.97%\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋 로더 생성\n",
    "test_loader = DataLoader(dataset=test, batch_size=100, shuffle=False)\n",
    "\n",
    "# 평가 실행\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33734fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))  # 배치 하나 가져오기\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for i in range(5):\n",
    "        img = images[i].squeeze()  # 1x28x28 → 28x28\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'Pred: {predicted[i].item()}\\nLabel: {labels[i].item()}')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07b0d67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_predictions(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
